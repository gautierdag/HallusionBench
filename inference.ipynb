{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b418577-15e1-4bc1-9819-3441bf18421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from PIL import Image\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"JotDe/mscoco_20k_unique_imgs\")\n",
    "# dataset = load_dataset(\"JotDe/mscoco_20k_unique_imgs\")\n",
    "dataset = load_dataset(\"google/docci\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a52526-dec9-49e5-8bfe-88ff47de1839",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'BAAI/Bunny-v1_1-Llama-3-8B-V' # or 'BAAI/Bunny-Llama-3-8B-V' or 'BAAI/Bunny-v1_1-4B' or 'BAAI/Bunny-v1_0-4B' or 'BAAI/Bunny-v1_0-3B' or 'BAAI/Bunny-v1_0-3B-zh' or 'BAAI/Bunny-v1_0-2B-zh'\n",
    "offset_bos = 1 # for Bunny-v1_1-Llama-3-8B-V, Bunny-Llama-3-8B-V, Bunny-v1_1-4B, Bunny-v1_0-4B and Bunny-v1_0-3B-zh\n",
    "# offset_bos = 0 for Bunny-v1_0-3B and Bunny-v1_0-2B-zh\n",
    "\n",
    "# create model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16, # float32 for cpu\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7c5661-f305-410a-86f3-669853564d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concat_h(im1, im2):\n",
    "    dst = Image.new('RGB', (im1.width + im2.width, im1.height))\n",
    "    dst.paste(im1, (0, 0))\n",
    "    dst.paste(im2, (im1.width, 0))\n",
    "    return dst\n",
    "\n",
    "def get_concat_v(im1, im2):\n",
    "    dst = Image.new('RGB', (im1.width, im1.height + im2.height))\n",
    "    dst.paste(im1, (0, 0))\n",
    "    dst.paste(im2, (0, im1.height))\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e281f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5648fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def get_new_word(word: str):\n",
    "    prompt = f\"{word}\"\n",
    "    text = f\"Only reply with a single word. Replace the word to another random class in COCO. USER: horse ASSISTANT: elephant USER: {prompt} ASSISTANT:\"\n",
    "    encoded_text = tokenizer(text, return_tensors=\"pt\")\n",
    "    encoded_text = {k: v.to(model.device) for k, v in encoded_text.items()}\n",
    "    # generate\n",
    "    output_ids = model.generate(\n",
    "        **encoded_text,\n",
    "        max_new_tokens=32,\n",
    "        use_cache=True,\n",
    "        do_sample=False,\n",
    "    )[0]\n",
    "    # decode\n",
    "    output_text = tokenizer.decode(output_ids, skip_special_tokens=True).split(\n",
    "        \"ASSISTANT: \"\n",
    "    )[-1]\n",
    "    return output_text\n",
    "\n",
    "\n",
    "def evaluate_image(image, caption):\n",
    "    # prompt = f\"Does the description '{caption}' match the given image? What does the image show instead?\"# Reply with 'yes' if it matches else 'no'.\"\n",
    "    prompt = f\"Does the description '{caption}' match the given image? Reply with 'yes' if it matches else 'no'.\"\n",
    "    # prompt = \"Caption the given image in a short sentence.\"\n",
    "    text = f\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image>\\n{prompt} ASSISTANT:\"\n",
    "    # text = f\"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER:{prompt} ASSISTANT: The image shows\"\"\"\n",
    "    text_chunks = [tokenizer(chunk).input_ids for chunk in text.split(\"<image>\")]\n",
    "    # encoded_text = tokenizer(text, return_tensors=\"pt\")\n",
    "    # encoded_text = {k: v.to(model.device) for k, v in encoded_text.items()}\n",
    "    input_ids = (\n",
    "        torch.tensor(\n",
    "            text_chunks[0] + [-200] + text_chunks[1][offset_bos:], dtype=torch.long\n",
    "        )\n",
    "        .unsqueeze(0)\n",
    "        .to(\"cuda\")\n",
    "    )\n",
    "    image_tensor = model.process_images([image], model.config).to(\n",
    "        dtype=model.dtype, device=model.device\n",
    "    )\n",
    "    # generate\n",
    "    output_ids = model.generate(\n",
    "        # **encoded_text,\n",
    "        input_ids,\n",
    "        images=[image_tensor],\n",
    "        max_new_tokens=16,\n",
    "        use_cache=True,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.0,  # increase this to avoid chattering\n",
    "    )[0]\n",
    "    # decode\n",
    "    output_text = tokenizer.decode(output_ids, skip_special_tokens=True).split(\n",
    "        \"ASSISTANT: \"\n",
    "    )[-1]\n",
    "    return output_text\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    example_A = random.choice(dataset[\"train\"])\n",
    "    example_B = random.choice(dataset[\"train\"])\n",
    "    # random_image = random.choice(dataset[\"train\"])[\"image\"]\n",
    "    doc_A = nlp(example_A[\"description\"])\n",
    "    doc_B = nlp(example_B[\"description\"])\n",
    "\n",
    "    sentences_A = list(doc_A.sents)\n",
    "    sentences_B = list(doc_B.sents)\n",
    "    random.shuffle(sentences_B)\n",
    "    random.shuffle(sentences_A)\n",
    "\n",
    "    # print(\"#\" * 100)\n",
    "    # print(\"\\n\".join([str(a) for a in sentences_A]))\n",
    "    # print(\"---\" * 100)\n",
    "    # print(\"\\n\".join([str(a) for a in sentences_B]))\n",
    "    # print(\"\\n\".join(sentences_B))\n",
    "    sentences = []\n",
    "    for s in sentences_A:\n",
    "        if random.random() < 0.8 and len(sentences_B) > 0:\n",
    "            sentence_from_B = sentences_B.pop(0)\n",
    "            sentences.append(sentence_from_B)\n",
    "        else:\n",
    "            sentences.append(s)\n",
    "    text = \" \".join([str(s) for s in sentences])\n",
    "\n",
    "    # print(doc)\n",
    "    # sentence = []\n",
    "    # for token in doc:\n",
    "    #     # Check if the token is a noun (POS: 'NOUN' or 'PROPN')\n",
    "    #     if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\" and random.random() < 0.1:\n",
    "    #         new_word = get_new_word(token.text)\n",
    "    #         sentence.append(new_word)\n",
    "    #     else:\n",
    "    #         sentence.append(token.text)\n",
    "    # new_caption = \" \".join(sentence)\n",
    "    # print(\"GENERATED CAPTION\")\n",
    "    # print(text.replace(\".\", \"\\n\"))\n",
    "    true_pred = evaluate_image(example_A[\"image\"], example_A[\"description\"])\n",
    "    pred = evaluate_image(example_A[\"image\"], text)\n",
    "    # true_pred = evaluate_image(random_image, example[\"text\"])\n",
    "    # pred = evaluate_image(random_image, new_caption)\n",
    "    print(f\"Is the original caption identified as matching: {true_pred}\")\n",
    "    print(f\"Is the modified caption identified as matching: {pred}\")\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836dcdac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40c16be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "\n",
    "example_paths = glob.glob(\"data/*.json\")\n",
    "\n",
    "results = []\n",
    "for example_path in example_paths:\n",
    "    with open(example_path) as f:\n",
    "        example = json.load(f)\n",
    "    example_idx = example_path.split(\"/\")[-1].split(\".json\")[0]\n",
    "    a_img = Image.open(\"data/imgs/\" + example_idx + \"_a.png\")\n",
    "    b_img = Image.open(\"data/imgs/\" + example_idx + \"_b.png\")\n",
    "    image = get_concat_h(a_img, b_img)\n",
    "    image_tensor = model.process_images([image], model.config).to(\n",
    "        dtype=model.dtype, device=\"cuda\"\n",
    "    )\n",
    "    # text prompt\n",
    "    for caption_origin in [\"A\", \"B\", \"A_B\", \"B_A\"]:\n",
    "        caption = example[caption_origin]\n",
    "        score = example[\"plausibility_scores\"][caption_origin]\n",
    "        print(caption_origin, caption)\n",
    "\n",
    "        prompt = f\"Description: {caption}\\n Does the description match any of the given images? Reply with yes or no.\"\n",
    "        text = f\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image>\\n{prompt} ASSISTANT:\"\n",
    "        text_chunks = [tokenizer(chunk).input_ids for chunk in text.split(\"<image>\")]\n",
    "        input_ids = (\n",
    "            torch.tensor(\n",
    "                text_chunks[0] + [-200] + text_chunks[1][offset_bos:], dtype=torch.long\n",
    "            )\n",
    "            .unsqueeze(0)\n",
    "            .to(\"cuda\")\n",
    "        )\n",
    "        # generate\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            images=image_tensor,\n",
    "            max_new_tokens=32,\n",
    "            use_cache=True,\n",
    "            do_sample=False,\n",
    "            repetition_penalty=1.0,  # increase this to avoid chattering\n",
    "        )[0]\n",
    "\n",
    "        # decode\n",
    "        output_text = tokenizer.decode(output_ids, skip_special_tokens=True).split(\n",
    "            \"ASSISTANT: \"\n",
    "        )[-1]\n",
    "        if \"yes\" in output_text.lower():\n",
    "            results.append(\n",
    "                (example_idx, caption, caption_origin, \"yes\", output_text, score)\n",
    "            )\n",
    "        elif \"no\" in output_text.lower():\n",
    "            results.append(\n",
    "                (example_idx, caption, caption_origin, \"no\", output_text, score)\n",
    "            )\n",
    "        else:\n",
    "            results.append(\n",
    "                (example_idx, caption, caption_origin, \"unknown\", output_text, score)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bed7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    results, columns=[\"example_idx\", \"caption\", \"caption_origin\", \"result\", \"output_text\", \"score\"]\n",
    ")\n",
    "df.loc[df[\"caption_origin\"].isin([\"A\", \"B\"])].result.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffacd373",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"caption_origin\"].isin([\"A\", \"B\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1ccd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"caption_origin\"].isin([\"A_B\", \"B_A\"])].result.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d024ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"caption_origin\"].isin([\"A_B\", \"B_A\"])&df[\"result\"].isin([\"yes\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa48c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df[\"correct\"] = 0\n",
    "df.loc[df[\"caption_origin\"].isin([\"A\", \"B\"]) & (df[\"result\"] == \"yes\"), \"correct\"] = 1\n",
    "df.loc[\n",
    "    df[\"caption_origin\"].isin([\"A_B\", \"B_A\"]) & (df[\"result\"] == \"no\"), \"correct\"\n",
    "] = 1\n",
    "\n",
    "df[\"fake_caption\"] = df[\"caption_origin\"].isin([\"A_B\", \"B_A\"])\n",
    "\n",
    "sns.barplot(x=\"fake_caption\", y=\"correct\", data=df, hue=\"score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c15741",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"caption_origin\"].isin([\"A\", \"B\"])].output_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dce0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_img = Image.open(\"data/imgs/\" + str(9) + \"_a.png\")\n",
    "# b_img = Image.open(\"data/imgs/\" + str(9) + \"_b.png\")\n",
    "# image = get_concat_h(a_img, b_img)\n",
    "# image\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
